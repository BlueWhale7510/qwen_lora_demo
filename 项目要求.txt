AI 应用开发工程师 面试考察实践题
通用提交要求

- 代码库： 提交一个结构清晰、文档完善的Git仓库链接。我们期望看到生产级别的代码质量，包括合理的模块划分、清晰的注释和必要的单元测试。
- README.md： 详尽的说明文档，包含：
  - 设计哲学： 阐述您的整体架构设计、关键技术选型及其原因（例如，为什么选择某种Agent类型、文本切分策略或评估方法）。
  - 环境与运行： 清晰的环境配置、依赖安装 (requirements.txt) 和项目运行指南。
  - 成果展示： 如何复现您的核心成果的示例或命令。
- LLM API： 请自行选择并使用一款您熟悉的大模型API，并在文档中注明。对于某些任务，可能需要使用支持Function Calling或Tool Using的模型。
- 完成实践题后发送思考过程到 markyan@jonlantech.com 。

题目：构建并评估一个“特定指令”微调模型

场景：
在许多业务场景中，通用大模型的输出风格或任务能力无法完全满足要求。通过在高质量的小数据集上进行微调（Fine-tuning），可以获得远超提示工程（Prompt Engineering）的效果。您需要完整地走一遍“数据准备->模型微调->效果评估”的流程。
任务：
针对“将复杂的法律术语翻译成通俗易懂的大白话”这一特定任务，微调一个开源小模型，并量化评估其效果。
输入数据：
您需要自行构建一个小型、高质量的数据集（至少20-30条样本）。每条样本包含：
- instruction: "请将以下法律术语解释成普通人能听懂的大白话。"
- input: 一段包含法律术语的句子（例如：“本合同的不可抗力条款旨在规避缔约双方在无法预见、无法避免且无法克服的客观情况下的违约责任。”）
- output: 对应的通俗解释（例如：“这份合同里有个条款说，如果发生了谁也想不到、躲不开也解决不了的大事（比如大地震），导致没法按合同办事，那双方都不算违约，不用赔钱。”）
产出要求：
1. 数据构建与准备： 创建并提交您的finetune_dataset.jsonl数据集。
2. 模型微调：
  - 选择一个合适的开源小模型（如Qwen-1.8B, ChatGLM3-6B的某个版本，或Llama-3-8B等）。
  - 使用LoRA或QLoRA等高效微调技术，完成模型的微调过程。
  - 提交您的微调训练脚本。
3. 效果评估：
  - 建立评估基线(Baseline)： 设计一个您认为最好的Zero-shot Prompt，让原始的、未微调的基础模型来完成同样的翻译任务。
  - 对比评估： 从您的数据集中划分出一个小的测试集（5条即可）。分别用“微调后的模型”和“带Prompt的基础模型”来生成回答。
  - 量化分析： 请使用一个更强的模型（如GPT-4/Kimi）作为裁判，设计一个评估Prompt，让它从“忠实原文”、“通俗易懂”两个维度，对两种方案生成的回答进行打分（1-5分），并给出优劣判断。
4. 报告分析： 在您的REPORT.md中，清晰地展示您的评估结果，并分析为什么微调后的模型在这一特定任务上可能表现得更好。

示例输出格式:
[
  {
    "instruction": "请将以下法律术语解释成普通人能听懂的大白话。",
    "input": "本合同的不可抗力条款旨在规避缔约双方在无法预见、无法避免且无法克服的客观情况下的违约责任。", 
    "output": "这份合同里有个条款说，如果发生了谁也想不到、躲不开也解决不了的大事（比如大地震），导致没法按合同办事，那双方都不算违约，不用赔钱。"
  },
  {
    "instruction": "请将以下法律术语解释成普通人能听懂的大白话。", 
    "input": "原告应承担举证责任，否则将面临败诉风险。", 
    "output": "谁告状谁就得拿出证据来证明自己说得对，要是拿不出证据，很可能就会输掉官司。"
  }
]

请生成符合上述要求的50个训练样本和5个验证样本。