"""
æ³•å¾‹æœ¯è¯­ç¿»è¯‘æ¨¡å‹è¯„ä¼°è„šæœ¬
å¯¹æ¯”åŸºç¡€æ¨¡å‹ï¼ˆZero-shotï¼‰ä¸å¾®è°ƒæ¨¡å‹ï¼ˆLoRAï¼‰çš„æ•ˆæœ
"""

import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def load_models():
    """åŠ è½½åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹"""
    base_model_name = "./qwen1.5-0.5b-model"
    
    print("=" * 60)
    print("åŠ è½½æ¨¡å‹ä¸­...")
    print("=" * 60)
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print("1. åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_tokenizer = AutoTokenizer.from_pretrained(
        base_model_name, 
        trust_remote_code=True,
        padding_side="left"
    )
    base_tokenizer.pad_token = base_tokenizer.eos_token
    
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    base_model.eval()
    print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½å¾®è°ƒæ¨¡å‹
    print("2. åŠ è½½å¾®è°ƒæ¨¡å‹...")
    finetuned_model = PeftModel.from_pretrained(
        AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        ),
        "./legal_translate_optimized"
    )
    finetuned_model.eval()
    print("âœ… å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ")
    
    return base_model, base_tokenizer, finetuned_model, base_tokenizer

def generate_baseline_answer(model, tokenizer, legal_text):
    """
    åŸºç¡€æ¨¡å‹ç”Ÿæˆ - ä½¿ç”¨ä¼˜åŒ–çš„Zero-shot Prompt
    """
    # ä¼˜åŒ–çš„Zero-shot Prompt
    prompt = f"""è¯·å°†ä»¥ä¸‹æ³•å¾‹æœ¯è¯­ç”¨æ™®é€šäººèƒ½å¬æ‡‚çš„å¤§ç™½è¯è§£é‡Šæ¸…æ¥šï¼Œè¦æ±‚ï¼š
1. å‡†ç¡®åæ˜ åŸæ„
2. ä½¿ç”¨æ—¥å¸¸å£è¯­è¡¨è¾¾
3. é¿å…ä¸“ä¸šæœ¯è¯­

æ³•å¾‹æœ¯è¯­ï¼š{legal_text}

é€šä¿—è§£é‡Šï¼š"""
    
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        truncation=True, 
        max_length=512
    )
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1,
            top_p=0.9
        )
    
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå–promptä¹‹åçš„å†…å®¹
    if prompt in decoded:
        return decoded[len(prompt):].strip()
    return decoded.strip()

def generate_finetuned_answer(model, tokenizer, legal_text):
    """
    å¾®è°ƒæ¨¡å‹ç”Ÿæˆ - ä½¿ç”¨è®­ç»ƒæ—¶çš„æ ¼å¼
    """
    input_text = f"### æŒ‡ä»¤:\nè¯·å°†ä»¥ä¸‹æ³•å¾‹æœ¯è¯­è§£é‡Šæˆæ™®é€šäººèƒ½å¬æ‡‚çš„å¤§ç™½è¯ã€‚\n\n### è¾“å…¥:\n{legal_text}\n\n### è¾“å‡º:\n"
    
    inputs = tokenizer(
        input_text, 
        return_tensors="pt", 
        truncation=True, 
        max_length=512
    )
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1,
            top_p=0.9
        )
    
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    if input_text in decoded:
        return decoded[len(input_text):].strip()
    return decoded.strip()

def load_test_data():
    """ä»è®­ç»ƒæ•°æ®ä¸­æå–5æ¡ä½œä¸ºæµ‹è¯•é›†"""
    with open('finetune_dataset.json', 'r', encoding='utf-8') as f:
        all_data = json.load(f)
    
    # é€‰æ‹©5æ¡ä½œä¸ºæµ‹è¯•é›†ï¼ˆé€‰æ‹©ä¸åŒæ³•å¾‹é¢†åŸŸçš„æ ·æœ¬ï¼‰
    test_indices = [0, 10, 20, 30, 40]  # å‡åŒ€åˆ†å¸ƒé€‰æ‹©
    test_data = [all_data[i] for i in test_indices if i < len(all_data)]
    
    print(f"ä»æ•°æ®é›†ä¸­é€‰å–äº† {len(test_data)} æ¡æµ‹è¯•æ ·æœ¬")
    return test_data

def main():
    # åŠ è½½æ¨¡å‹
    base_model, base_tokenizer, finetuned_model, finetuned_tokenizer = load_models()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_test_data()
    
    print("\n" + "=" * 80)
    print("æ³•å¾‹æœ¯è¯­ç¿»è¯‘æ¨¡å‹å¯¹æ¯”è¯„ä¼°")
    print("=" * 80)
    
    results = []
    
    for i, sample in enumerate(test_data, 1):
        legal_text = sample['input']
        reference = sample['output']
        
        print(f"\nğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ {i}:")
        print(f"ã€æ³•å¾‹æœ¯è¯­ã€‘: {legal_text}")
        print(f"ã€å‚è€ƒè¾“å‡ºã€‘: {reference}")
        
        # åŸºç¡€æ¨¡å‹ç”Ÿæˆ
        print(f"\nğŸ”µ ã€åŸºç¡€æ¨¡å‹ - Zero-shotã€‘:")
        baseline_output = generate_baseline_answer(base_model, base_tokenizer, legal_text)
        print(f"   {baseline_output}")
        
        # å¾®è°ƒæ¨¡å‹ç”Ÿæˆ
        print(f"\nğŸŸ¢ ã€å¾®è°ƒæ¨¡å‹ - LoRAã€‘:")
        finetuned_output = generate_finetuned_answer(finetuned_model, finetuned_tokenizer, legal_text)
        print(f"   {finetuned_output}")
        
        # ä¿å­˜ç»“æœ
        result = {
            "sample_id": i,
            "legal_text": legal_text,
            "reference": reference,
            "baseline_output": baseline_output,
            "finetuned_output": finetuned_output
        }
        results.append(result)
        
        print("-" * 80)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open("baseline_comparison.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆç®€æ´çš„å¯¹æ¯”æŠ¥å‘Š
    generate_comparison_report(results)
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“Š è¯¦ç»†ç»“æœä¿å­˜è‡³: baseline_comparison.json")
    print(f"ğŸ“‹ å¯¹æ¯”æŠ¥å‘Šä¿å­˜è‡³: comparison_report.txt")

def generate_comparison_report(results):
    """ç”Ÿæˆç®€æ´çš„å¯¹æ¯”æŠ¥å‘Š"""
    report = ["æ³•å¾‹æœ¯è¯­ç¿»è¯‘æ¨¡å‹å¯¹æ¯”è¯„ä¼°æŠ¥å‘Š", "=" * 50, ""]
    
    for result in results:
        report.append(f"æ ·æœ¬ {result['sample_id']}:")
        report.append(f"æ³•å¾‹æœ¯è¯­: {result['legal_text']}")
        report.append(f"å‚è€ƒè¾“å‡º: {result['reference']}")
        report.append("")
        report.append("åŸºç¡€æ¨¡å‹ (Zero-shot):")
        report.append(f"  {result['baseline_output']}")
        report.append("")
        report.append("å¾®è°ƒæ¨¡å‹ (LoRA):")
        report.append(f"  {result['finetuned_output']}")
        report.append("-" * 60)
        report.append("")
    
    report.append("è¯„ä¼°è¯´æ˜:")
    report.append("1. å¿ å®åŸæ–‡ - è§£é‡Šæ˜¯å¦å‡†ç¡®åæ˜ æ³•å¾‹å«ä¹‰")
    report.append("2. é€šä¿—æ˜“æ‡‚ - æ˜¯å¦ä½¿ç”¨æ™®é€šäººèƒ½ç†è§£çš„è¯­è¨€") 
    report.append("3. è¡¨è¾¾è‡ªç„¶ - è¯­è¨€æ˜¯å¦æµç•…è‡ªç„¶")
    report.append("")
    report.append("è¯·ä»ä»¥ä¸Šä¸‰ä¸ªç»´åº¦å¯¹ä¸¤ç§æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œè¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰")
    
    with open("comparison_report.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(report))
    
    # åŒæ—¶åœ¨æ§åˆ¶å°è¾“å‡ºç®€æ´ç‰ˆæœ¬
    print("\n" + "=" * 60)
    print("ç®€æ´å¯¹æ¯”ç»“æœ:")
    print("=" * 60)
    
    for result in results:
        print(f"\næ¡ˆä¾‹ {result['sample_id']}: {result['legal_text']}")
        print(f"å‚è€ƒ: {result['reference']}")
        print(f"åŸºç¡€: {result['baseline_output'][:80]}...")
        print(f"å¾®è°ƒ: {result['finetuned_output'][:80]}...")

if __name__ == "__main__":
    main()